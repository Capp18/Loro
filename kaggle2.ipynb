{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaggle2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgUjCN5IBuTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path=\"/content/drive/My Drive/Colab Notebooks/\"\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCg6IP2hM068",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "outputId": "15f7a695-9e81-4cd3-a579-d4e1a9e2b96f"
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "# # Home Credit Default Risk\n",
        "\n",
        "# ## Predicting how capable each applicant is of repaying a loan?\n",
        "\n",
        "# ![home%20credit.jpg](attachment:home%20credit.jpg)\n",
        "\n",
        "# Introduction: Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.\n",
        "# \n",
        "# Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n",
        "# \n",
        "# While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.\n",
        "# \n",
        "# https://www.kaggle.com/c/home-credit-default-risk\n",
        "# \n",
        "# The objective of this competition is to use historical loan application data to predict whether or not an applicant will be able to repay a loan. This is a standard supervised classification task:\n",
        "# \n",
        "# Supervised: The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features.\n",
        "# \n",
        "# Classification: The label is a binary variable, 0 (will repay loan on time), 1 (will have difficulty repaying loan)\n",
        "\n",
        "# ## Import necessary libraries.\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "# import numpy for math calculations\n",
        "import numpy as np\n",
        "\n",
        "# import pandas for data (csv) manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# import matplotlib for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import seaborn for more plotting options(built on top of matplotlib)\n",
        "import seaborn as sns\n",
        "\n",
        "import pandas as pd\n",
        "# Supress unnecessary warnings so that the presentation looks clean\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# display plots on the notebook itself\n",
        "get_ipython().magic('matplotlib inline')\n",
        "\n",
        "\n",
        "# ## Read the data files\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "train = pd.read_csv(path+\"application_train.csv\")\n",
        "test = pd.read_csv(path+\"application_test.csv\")\n",
        "\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "train.info()\n",
        "\n",
        "\n",
        "# ## How is the statistic?\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "train.describe()\n",
        "\n",
        "\n",
        "# ## How are the target labels spread?\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "sns.countplot(train.TARGET)\n",
        "\n",
        "\n",
        "# In[6]:\n",
        "\n",
        "\n",
        "train['TARGET'].value_counts()\n",
        "\n",
        "\n",
        "# ### This is clearly an imbalanced target. There are more number of people who returned - 0 as opposed to people who had difficulties -1. About 91.92 % of applicants repayed!\n",
        "\n",
        "# ## What are the dimensions of Train and Test dataset?\n",
        "\n",
        "# In[7]:\n",
        "\n",
        "\n",
        "print(\"The train dataset dimensions are as follows: {}\".format(train.shape))\n",
        "print(\"The test dataset dimensions are as follows: {}\".format(test.shape))\n",
        "\n",
        "\n",
        "# ## Look at the train dataset\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "train.head()\n",
        "\n",
        "\n",
        "# ## Look at the test dataset\n",
        "\n",
        "# In[9]:\n",
        "\n",
        "\n",
        "test.head()\n",
        "\n",
        "\n",
        "# ## Look at the New Test dataset\n",
        "\n",
        "# In[10]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #### As expected, test dataset contains all the columns except the target label.\n",
        "\n",
        "# ## What are the missing values and their column names?\n",
        "\n",
        "# In[11]:\n",
        "\n",
        "\n",
        "def missing_columns(dataframe):\n",
        "    \"\"\"\n",
        "    Returns a dataframe that contains missing column names and \n",
        "    percent of missing values in relation to the whole dataframe.\n",
        "    \n",
        "    dataframe: dataframe that gives the column names and their % of missing values\n",
        "    \"\"\"\n",
        "    \n",
        "    # find the missing values\n",
        "    missing_values = dataframe.isnull().sum().sort_values(ascending=False)\n",
        "    \n",
        "    # percentage of missing values in relation to the overall size\n",
        "    missing_values_pct = 100 * missing_values/len(dataframe)\n",
        "    \n",
        "    # create a new dataframe which is a concatinated version\n",
        "    concat_values = pd.concat([missing_values, missing_values/len(dataframe),missing_values_pct.round(1)],axis=1)\n",
        "\n",
        "    # give new col names\n",
        "    concat_values.columns = ['Missing Count','Missing Count Ratio','Missing Count %']\n",
        "    \n",
        "    # return the required values\n",
        "    return concat_values[concat_values.iloc[:,1]!=0]\n",
        "    \n",
        "\n",
        "\n",
        "# In[12]:\n",
        "\n",
        "\n",
        "missing_columns(train)\n",
        "\n",
        "\n",
        "# In[13]:\n",
        "\n",
        "\n",
        "missing_columns(test)\n",
        "\n",
        "\n",
        "# In[14]:\n",
        "\n",
        "\n",
        "\n",
        "# We will have to handle these missing values (known as imputation). Other option would be to drop all those columns where there are large number of missing values. Unless we know the feature importance, it is not possible to make a call on which columns to keep which ones to drop.\n",
        "\n",
        "# ## What are the different datatypes of columns? - How many floats, integers, categoricals?\n",
        "\n",
        "# In[15]:\n",
        "\n",
        "\n",
        "print(\"Train dataset: \\n{}\".format(train.dtypes.value_counts()))\n",
        "print()\n",
        "print(\"Test dataset: \\n{}\".format(test.dtypes.value_counts())) \n",
        "print()\n",
        "\n",
        "\n",
        "# #### Turn every column data type of testing set similar to training set. Match datatypes of test in alignment with train. \n",
        "\n",
        "# In[6]:\n",
        "\n",
        "\n",
        "def match_dtypes(training_df,testing_df,target_name='TARGET'):\n",
        "    \"\"\"\n",
        "    This function converts dataframe to match columns in accordance with the \n",
        "    training dataframe.\n",
        "    \"\"\"\n",
        "    for column_name in training_df.drop([target_name],axis=1).columns:\n",
        "         testing_df[column_name]= testing_df[column_name].astype(train[column_name].dtype)\n",
        "        \n",
        "    return testing_df\n",
        "    \n",
        "\n",
        "\n",
        "# In[7]:\n",
        "\n",
        "\n",
        "\n",
        "# In[18]:\n",
        "\n",
        "\n",
        "print(\"Train dataset: \\n{}\".format(train.dtypes.value_counts()))\n",
        "print()\n",
        "print(\"Test dataset: \\n{}\".format(test.dtypes.value_counts())) \n",
        "print()\n",
        "\n",
        "\n",
        "# ### In test dataset, 40 int64 indicates that the target label is missing - which is obvious.\n",
        "\n",
        "# ### What are the different kinds of classes in every categorical column?\n",
        "\n",
        "# In[19]:\n",
        "\n",
        "\n",
        "# Number of unique classes in each object column\n",
        "train.select_dtypes('object').apply(pd.Series.nunique)\n",
        "\n",
        "\n",
        "# In[20]:\n",
        "\n",
        "\n",
        "test.select_dtypes('object').apply(pd.Series.nunique)\n",
        "\n",
        "\n",
        "# In[21]:\n",
        "\n",
        "\n",
        "\n",
        "# ## Handling Categorical variables - Label Encoding and One Hot Encoding.\n",
        "\n",
        "# Some machine learning models can't learn if provided with text categories. The categorical variables are to be converted into\n",
        "# numerical equivalent, which is done by Label encoding and One hot encoding.\n",
        "# \n",
        "# <b>Label encoding:</b> It is the process of assigning each unique category in a categorical variable with an integer. No new columns are created. \n",
        "\n",
        "# ![label_encoding.png](attachment:label_encoding.png)\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "# Create a label encode object having less than or equal to 2 unique values\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "transform_counter = 0\n",
        "\n",
        "# iterate through all the categorical columns\n",
        "for col in train.select_dtypes('object').columns:\n",
        "    \n",
        "    # select only those columns where number of unique values in the category is less than or equal to 2 \n",
        "    if pd.Series.nunique(train[col]) <= 2:\n",
        "        train[col] = le.fit_transform(train[col].astype(str))\n",
        "        test[col] = le.fit_transform(test[col].astype(str))\n",
        "\n",
        "        transform_counter+=1\n",
        "        \n",
        "print(\"Label encoded {} columns.\".format(transform_counter))    \n",
        "\n",
        "\n",
        "# <b>One-hot encoding:</b> create a new column for each unique category in a categorical variable. Each observation recieves a 1 in the column for its corresponding category and a 0 in all other new columns.\n",
        "\n",
        "# ![one%20hot1.jpg](attachment:one%20hot1.jpg)\n",
        "# Credit : https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f\n",
        "\n",
        "# In[9]:\n",
        "\n",
        "\n",
        "# one-hot encode of categorical variables\n",
        "train = pd.get_dummies(train,drop_first=True)\n",
        "test = pd.get_dummies(test,drop_first=True)\n",
        "\n",
        "\n",
        "\n",
        "# One hot encoding would added more columns, checking how many there are: \n",
        "\n",
        "# In[10]:\n",
        "\n",
        "\n",
        "print('Training Features shape: ', train.shape)\n",
        "print('Testing Features shape: ', test.shape)\n",
        "\n",
        "\n",
        "# There is a mismatch in the count of columns for test and train. This can be fixed by aligning them.\n",
        "\n",
        "# In[11]:\n",
        "\n",
        "\n",
        "# collect the target labels to support the aligning \n",
        "\n",
        "target = train['TARGET']\n",
        "\n",
        "\n",
        "# ## Ensure train and test have the same number of columns by aligning.\n",
        "\n",
        "# In[12]:\n",
        "\n",
        "\n",
        "train, test = train.align(test,axis=1,join='inner')\n",
        "\n",
        "\n",
        "# Add the stored target column back into the train dataset.\n",
        "\n",
        "# In[13]:\n",
        "\n",
        "\n",
        "train['TARGET'] = target\n",
        "\n",
        "\n",
        "# Since there are extra columns in the training set and those columns are missing in the new_testing set, let us add those columns and assign them to dummy value of 0.\n",
        "\n",
        "# In[14]:\n",
        "\n",
        "\n",
        "def match_columns(training_set,testing_set,target_label='TARGET'):\n",
        "    \"\"\"Matches the count of columns from training set to testing set by adding extra cols and setting them to 0.\"\"\"\n",
        "    \n",
        "    for column in training_set.drop([target_label],axis=1).columns:\n",
        "        if column not in testing_set.columns:\n",
        "            testing_set[column]=0\n",
        "    \n",
        "    return testing_set        \n",
        "\n",
        "\n",
        "# In[15]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# In[16]:\n",
        "\n",
        "\n",
        "print('Training Features shape: ', train.shape)\n",
        "print('Testing Features shape: ', test.shape)\n",
        "\n",
        "\n",
        "\n",
        "# <h3>On the look for Anomalies</h3> \n",
        "# </br>\n",
        "# \n",
        "# One problem we always want to be on the lookout for is anomalies within the data. These may be due to mis-typed numbers, errors in measuring equipment, or they could be valid but extreme measurements. One way to support anomalies checking is by looking at the statistics of a column using the describe method. The numbers in the DAYS_BIRTH column are negative because they are recorded relative to the current loan application. To see these stats in years, we can multiply by -1 and divide by the number of days in a year:\n",
        "\n",
        "# ## How old are clients?\n",
        "\n",
        "# In[31]:\n",
        "\n",
        "\n",
        "(train['DAYS_BIRTH']/-365).describe()\n",
        "\n",
        "\n",
        "# Ages seem to be fine, nothing in particluar seems to be off.\n",
        "\n",
        "# In[32]:\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(12,7))\n",
        "sns.distplot(train['DAYS_BIRTH']/-365,bins=5,kde=False)\n",
        "plt.xlabel(\"Age of the client (Years)\")\n",
        "\n",
        "\n",
        "# People in the age range 30-40 years are the most applicants. Which seems pretty normal.\n",
        "\n",
        "# ### How many years has it been since the applicant started working? \n",
        "# The DAYS_EMPLOYED column is negative because the days are relative only to the time of the application. -ve means so many days since the application, the client has been working. +ve means, the client is about to work in those many days. In an ideal world, the -ve has significance, +ve could mean anything from client starts working to client can be fired and resumes working, which in anyway doesn't make sense because the loan might not be given to those clients without any work.\n",
        "\n",
        "# In[33]:\n",
        "\n",
        "\n",
        "(train['DAYS_EMPLOYED']/365).describe()\n",
        "\n",
        "\n",
        "# This doesn't seem right, the maximum value (besides being positive) is about 1000 years!\n",
        "\n",
        "# ### Who are these special people who got employed 1000 years after issuance of the loan? \n",
        "\n",
        "# In[34]:\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,7))\n",
        "sns.distplot(train['DAYS_EMPLOYED']/365,kde=False)\n",
        "plt.xlabel(\"Time before the loan application the persons started current employment(in years)\")\n",
        "\n",
        "\n",
        "# So, how many of these 1000 year anomalies?\n",
        "\n",
        "# In[17]:\n",
        "\n",
        "\n",
        "# find the number of records where DAYS_EMPLOYED is between [900,1100] years. \n",
        "thousand_anomalies = train[(train['DAYS_EMPLOYED']/365>=900) & (train['DAYS_EMPLOYED']/365<=1100)]\n",
        "len(thousand_anomalies)\n",
        "\n",
        "\n",
        "# ## Lets look their ability to repay.\n",
        "\n",
        "# In[36]:\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,7))\n",
        "sns.countplot(x='TARGET',data=thousand_anomalies)\n",
        "\n",
        "\n",
        "# ## Most anomalies were able to repay on time. But how can they be contrasted in relation to non anomalies?\n",
        "\n",
        "# In[18]:\n",
        "\n",
        "\n",
        "# get the index of anomalies and non anomalies\n",
        "anomalies_index = pd.Index(thousand_anomalies.index)\n",
        "non_anomalies_index = train.index.difference(anomalies_index)\n",
        "\n",
        "\n",
        "# In[19]:\n",
        "\n",
        "\n",
        "# get the anomalies records\n",
        "non_anomalies = train.iloc[non_anomalies_index]\n",
        "\n",
        "\n",
        "# In[20]:\n",
        "\n",
        "\n",
        "# get the anomaly targets\n",
        "anomalies_target = thousand_anomalies['TARGET'].value_counts()\n",
        "non_anomalies_target = non_anomalies['TARGET'].value_counts()\n",
        "\n",
        "\n",
        "# In[21]:\n",
        "\n",
        "\n",
        "# find the default rate for anomalies and non anomalies\n",
        "\n",
        "print(\"Anomalies have a default rate of {}%\".format(100*anomalies_target[1]/(anomalies_target[1]+anomalies_target[0])))\n",
        "print(\"Non Anomalies have a default rate of {}%\".format(100*non_anomalies_target[1]/(non_anomalies_target[1]+non_anomalies_target[0])))\n",
        "\n",
        "\n",
        "# So surprisingly anomalies have lesser default rate!\n",
        "\n",
        "# Handling the anomalies depends on the exact situation, with no set rules. One of the safest approaches is just to set the anomalies to a missing value and then have them filled in (using Imputation) before machine learning. In this case, since all the anomalies have the exact same value, we want to fill them in with the same value in case all of these loans share something in common. The anomalous values seem to have some importance, so we want to tell the machine learning model if we did in fact fill in these values. As a solution, we will fill in the anomalous values with not a number (np.nan) and then create a new boolean column indicating whether or not the value was anomalous.\n",
        "\n",
        "# In[22]:\n",
        "\n",
        "\n",
        "# Create an anomalous flag column\n",
        "train['DAYS_EMPLOYED_ANOM'] = train[\"DAYS_EMPLOYED\"] == 365243\n",
        "\n",
        "# Replace the anomalous values with nan\n",
        "train['DAYS_EMPLOYED'] = train['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
        "\n",
        "\n",
        "# In[23]:\n",
        "\n",
        "\n",
        "# Looking at the years employed for anomalies\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "(train['DAYS_EMPLOYED']/-365).plot.hist(title = 'Years Employment Histogram')\n",
        "plt.xlabel(\"Years worked before application\")\n",
        "\n",
        "\n",
        "# Now it all seems normal!\n",
        "\n",
        "# In[24]:\n",
        "\n",
        "\n",
        "# Create an anomalous flag column\n",
        "test['DAYS_EMPLOYED_ANOM'] = test[\"DAYS_EMPLOYED\"] == 365243\n",
        "\n",
        "# Replace the anomalous values with nan\n",
        "test['DAYS_EMPLOYED'] = test['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
        "\n",
        "# Create an anomalous flag column\n",
        "\n",
        "\n",
        "\n",
        "# ## Finding out the most correlated features for the TARGET variable. \n",
        "\n",
        "# ## Understanding Correlation\n",
        "# \n",
        "# Correlation is a statistical measure that indicates the extent to which two or more variables fluctuate together. A positive correlation indicates the extent to which those variables increase or decrease in parallel; a negative correlation indicates the extent to which one variable increases as the other decreases.\n",
        "# \n",
        "# A correlation coefficient is a statistical measure of the degree to which changes to the value of one variable predict change to the value of another. When the fluctuation of one variable reliably predicts a similar fluctuation in another variable, there’s often a tendency to think that means that the change in one causes the change in the other. However, correlation does not imply causation. There may be, for example, an unknown factor that influences both variables similarly.\n",
        "# \n",
        "# ![correlation.png](attachment:correlation.png)\n",
        "# \n",
        "# To describe the strength of the\n",
        "# correlation using the guide that Evans (1996) suggests for the absolute value of r:\n",
        "# <br/>\n",
        "#  .00-.19 “very weak”\n",
        "#  <br/>\n",
        "#  .20-.39 “weak”\n",
        "#  <br/>\n",
        "#  .40-.59 “moderate”\n",
        "#  <br/>\n",
        "#  .60-.79 “strong”\n",
        "#  <br/>\n",
        "#  .80-1.0 “very strong”\n",
        "# \n",
        "# \n",
        "# \n",
        "# http://www.statstutor.ac.uk/resources/uploaded/pearsons.pdf <br/>\n",
        "# https://whatis.techtarget.com/definition/correlation\n",
        "\n",
        "# In[25]:\n",
        "\n",
        "\n",
        "corr_train = train.corr()['TARGET']\n",
        "\n",
        "\n",
        "# ## Looking at the top 10 most positively and negatively correlated features we get:\n",
        "\n",
        "# In[26]:\n",
        "\n",
        "\n",
        "print(corr_train.sort_values().tail(10))\n",
        "corr_train.sort_values().head(10)\n",
        "\n",
        "\n",
        "# ### Since EXT_SOURCE_3, EXT_SOURCE_2, EXT_SOURCE_1 and DAYS_BIRTH are highly correlated (Relatively), let us also explore the possibility of having them as interaction variables.\n",
        "# \n",
        "\n",
        "# ## Initially filling up the missing values for the most correlated variables.\n",
        "\n",
        "# In[27]:\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 307511 entries, 0 to 307510\n",
            "Columns: 122 entries, SK_ID_CURR to AMT_REQ_CREDIT_BUREAU_YEAR\n",
            "dtypes: float64(65), int64(41), object(16)\n",
            "memory usage: 286.2+ MB\n",
            "The train dataset dimensions are as follows: (307511, 122)\n",
            "The test dataset dimensions are as follows: (48744, 121)\n",
            "Train dataset: \n",
            "float64    65\n",
            "int64      41\n",
            "object     16\n",
            "dtype: int64\n",
            "\n",
            "Test dataset: \n",
            "float64    65\n",
            "int64      40\n",
            "object     16\n",
            "dtype: int64\n",
            "\n",
            "Train dataset: \n",
            "float64    65\n",
            "int64      41\n",
            "object     16\n",
            "dtype: int64\n",
            "\n",
            "Test dataset: \n",
            "float64    65\n",
            "int64      40\n",
            "object     16\n",
            "dtype: int64\n",
            "\n",
            "Label encoded 4 columns.\n",
            "Training Features shape:  (307511, 230)\n",
            "Testing Features shape:  (48744, 226)\n",
            "Training Features shape:  (307511, 227)\n",
            "Testing Features shape:  (48744, 226)\n",
            "Anomalies have a default rate of 5.399646043269405%\n",
            "Non Anomalies have a default rate of 8.659974537652149%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmOdzae3oZdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "# In[28]:\n",
        "\n",
        "\n",
        "poly_fitting_vars = ['EXT_SOURCE_3', 'EXT_SOURCE_2', 'EXT_SOURCE_1','DAYS_BIRTH']\n",
        "\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "\n",
        "\n",
        "# In[30]:\n",
        "\n",
        "train[poly_fitting_vars]=imputer.fit_transform(train[poly_fitting_vars])\n",
        "\n",
        "# In[31]:\n",
        "\n",
        "\n",
        "train[poly_fitting_vars].shape\n",
        "\n",
        "# In[32]:\n",
        "\n",
        "test[poly_fitting_vars]=imputer.fit_transform(test[poly_fitting_vars])\n",
        "\n",
        "\n",
        "# In[33]:\n",
        "\n",
        "\n",
        "test[poly_fitting_vars].shape\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "\n",
        "# In[37]:\n",
        "\n",
        "\n",
        "poly_feat = PolynomialFeatures(degree=4)\n",
        "\n",
        "\n",
        "# #### I also tried for polynomial degree of order 10. Couldn't find much improvement from degree 4 to 10. Fun fact: order of 10 created over 1000 interaction variables! \n",
        "\n",
        "# In[39]:\n",
        "\n",
        "\n",
        "poly_interaction_train = poly_feat.fit_transform(train[poly_fitting_vars])\n",
        "\n",
        "\n",
        "# In[40]:\n",
        "\n",
        "\n",
        "poly_interaction_train.shape\n",
        "\n",
        "\n",
        "# In[41]:\n",
        "\n",
        "\n",
        "poly_interaction_test = poly_feat.fit_transform(test[poly_fitting_vars])\n",
        "\n",
        "\n",
        "# In[42]:\n",
        "\n",
        "\n",
        "poly_interaction_test.shape\n",
        "\n",
        "\n",
        "# In[43]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# In[44]:\n",
        "\n",
        "\n",
        "# ## Build a dataframe out of interaction variables only!\n",
        "\n",
        "# In[45]:\n",
        "\n",
        "\n",
        "poly_interaction_train = pd.DataFrame(poly_interaction_train,columns=poly_feat.get_feature_names(poly_fitting_vars))\n",
        "\n",
        "\n",
        "# In[46]:\n",
        "\n",
        "\n",
        "poly_interaction_train.shape\n",
        "\n",
        "\n",
        "# In[47]:\n",
        "\n",
        "\n",
        "poly_interaction_test =  pd.DataFrame(poly_interaction_test,columns=poly_feat.get_feature_names(poly_fitting_vars))\n",
        "\n",
        "\n",
        "# In[48]:\n",
        "\n",
        "\n",
        "poly_interaction_test.shape\n",
        "\n",
        "\n",
        "\n",
        "# ## Add the 'TARGET' column which is later used for looking up correlations with the interaction variables.\n",
        "\n",
        "# In[51]:\n",
        "\n",
        "\n",
        "poly_interaction_train['TARGET'] = train['TARGET']\n",
        "\n",
        "\n",
        "# In[52]:\n",
        "\n",
        "\n",
        "interaction = poly_interaction_train.corr()['TARGET'].sort_values()\n",
        "\n",
        "\n",
        "# ## Which are the most correlated interaction variables?\n",
        "\n",
        "# In[53]:\n",
        "\n",
        "\n",
        "# looking at the top 15 most positive and negative correlated interaction variables.\n",
        "print(interaction.tail(15))\n",
        "(interaction.head(15))\n",
        "\n",
        "\n",
        "# ## Get the names of the columns which have the highest correlation - '1' and 'TARGET' can be dropped.\n",
        "\n",
        "# In[54]:\n",
        "\n",
        "\n",
        "set(interaction.head(15).index).union(interaction.tail(15).index).difference(set({'1','TARGET'}))\n",
        "\n",
        "\n",
        "# ## Choose the selected columns which have highest correlation to 'TARGET'. Columns '1' and 'TARGET' are not necessary!\n",
        "\n",
        "# In[55]:\n",
        "\n",
        "\n",
        "selected_inter_variables = list(set(interaction.head(15).index).union(interaction.tail(15).index).difference(set({'1','TARGET'})))\n",
        "\n",
        "\n",
        "# In[56]:\n",
        "\n",
        "\n",
        "# look at the selected features\n",
        "poly_interaction_train[selected_inter_variables].head()\n",
        "\n",
        "\n",
        "# In[57]:\n",
        "\n",
        "\n",
        "poly_interaction_test[selected_inter_variables].head()\n",
        "\n",
        "\n",
        "# In[58]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ## Get a list of unselected columns that are to be dropped.\n",
        "\n",
        "# In[59]:\n",
        "\n",
        "\n",
        "unselected_cols = [element for element in poly_interaction_train.columns if element not in selected_inter_variables]\n",
        "\n",
        "\n",
        "# ##  Drop the unselected columns of the interaction dataframes - train and test versions both.\n",
        "\n",
        "# In[60]:\n",
        "\n",
        "\n",
        "poly_interaction_train = poly_interaction_train.drop(unselected_cols,axis=1)\n",
        "\n",
        "\n",
        "# In[61]:\n",
        "\n",
        "\n",
        "poly_interaction_test = poly_interaction_test.drop(list(set(unselected_cols).difference({'TARGET'})),axis=1)\n",
        "\n",
        "\n",
        "# In[62]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ## Merge polynomial features into the original dataframes using their indices.\n",
        "\n",
        "# #### Dropping columns 'EXT_SOURCE_2' and 'EXT_SOURCE_3' since they're already present in the source dataset.\n",
        "\n",
        "# In[63]:\n",
        "\n",
        "\n",
        "train = train.join(poly_interaction_train.drop(['EXT_SOURCE_2', 'EXT_SOURCE_3'],axis=1))\n",
        "\n",
        "\n",
        "# In[64]:\n",
        "\n",
        "\n",
        "test = test.join(poly_interaction_test.drop(['EXT_SOURCE_2', 'EXT_SOURCE_3'],axis=1))\n",
        "\n",
        "\n",
        "# In[65]:\n",
        "\n",
        "\n",
        "\n",
        "# ## What are their merged dataframe dimensions?\n",
        "\n",
        "# In[66]:\n",
        "\n",
        "\n",
        "print(\"The train dataset dimensions are as follows: {}\".format(train.shape))\n",
        "print(\"The test dataset dimensions are as follows: {}\".format(test.shape))\n",
        "train['DIR'] = train['AMT_CREDIT']/train['AMT_INCOME_TOTAL']\n",
        "train['AIR'] = train['AMT_ANNUITY']/train['AMT_INCOME_TOTAL']\n",
        "train['ACR'] = train['AMT_ANNUITY']/train['AMT_CREDIT']\n",
        "train['DAR'] = train['DAYS_EMPLOYED']/train['DAYS_BIRTH']\n",
        "\n",
        "\n",
        "# In[68]:\n",
        "\n",
        "\n",
        "test['DIR'] = test['AMT_CREDIT']/test['AMT_INCOME_TOTAL']\n",
        "test['AIR'] = test['AMT_ANNUITY']/test['AMT_INCOME_TOTAL']\n",
        "test['ACR'] = test['AMT_ANNUITY']/test['AMT_CREDIT']\n",
        "test['DAR'] = test['DAYS_EMPLOYED']/test['DAYS_BIRTH']\n",
        "\n",
        "# In[69]:\n",
        "\n",
        "\n",
        "\n",
        "# ## Look at the correlation of the newly added variables in relation to the 'TARGET'\n",
        "\n",
        "# In[70]:\n",
        "\n",
        "\n",
        "corr_vals = train.corr()['TARGET']\n",
        "\n",
        "\n",
        "# In[71]:\n",
        "\n",
        "\n",
        "corr_vals.tail(4)\n",
        "\n",
        "\n",
        "# ## Hmmm, not much correlation - Linear!\n",
        "\n",
        "# # Preparing the dataset for feeding into the model.\n",
        "\n",
        "# ## Feature Imputing\n",
        "\n",
        "# Feature imputation is the process of filling up missed/NAN values for those columns where \n",
        "# certain cells are not filled by default due to reasons such as outlier replacement / unavailable data \n",
        "# or incorrect entires during capturing the data.   \n",
        "\n",
        "# In[72]:\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "# In[73]:\n",
        "\n",
        "\n",
        "features = list(set(train.columns).difference({'TARGET'}))\n",
        "\n",
        "\n",
        "# Imputation is done for the median value of every column.\n",
        "\n",
        "# In[74]:\n",
        "\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "\n",
        "\n",
        "# ## Feature Scaling\n",
        "# \n",
        "# Feature scaling is a method used to standardize the range of independent variables or features of data. \n",
        "# In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.\n",
        "#  \n",
        "# \n",
        "# Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. \n",
        "# For example, the majority of classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.\n",
        "# Another reason why feature scaling is applied is that gradient descent converges much faster with feature scaling than without it.\n",
        "\n",
        "# In[76]:\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "\n",
        "\n",
        "# In[77]:\n",
        "\n",
        "\n",
        "train_transformed = imputer.fit_transform(train.drop(['TARGET'],axis=1))\n",
        "\n",
        "test_transformed = imputer.transform(test)\n",
        "\n",
        "train_transformed = scaler.fit_transform(train_transformed)\n",
        "\n",
        "\n",
        "# In[82]:\n",
        "\n",
        "\n",
        "test_transformed = scaler.transform(test_transformed)\n",
        "\n",
        "\n",
        "# In[83]:\n",
        "\n",
        "\n",
        "\n",
        "# In[84]:\n",
        "\n",
        "\n",
        "# new_test[new_test.isnull().any(axis=1)]\n",
        "\n",
        "\n",
        "# In[85]:\n",
        "\n",
        "\n",
        "print(\"The train dataset dimensions are as follows: {}\".format(train_transformed.shape))\n",
        "print(\"The test dataset dimensions are as follows: {}\".format(test_transformed.shape))\n",
        "\n",
        "\n",
        "\n",
        "# In[89]:\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "\n",
        "# # Split the dataset into training set and validation set\n",
        "\n",
        "# In[104]:\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_training_set, X_validation_set, y_training_set, y_validation_set = train_test_split(train_transformed, \n",
        "                                                                                      target, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBlF5D9sM1b-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Starting with Logistic Regression.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logistic_regressor = LogisticRegression(C = 2)\n",
        "\n",
        "\n",
        "# In[106]:\n",
        "\n",
        "\n",
        "logistic_regressor.fit(X_training_set,y_training_set)\n",
        "\n",
        "\n",
        "# In[107]:\n",
        "\n",
        "\n",
        "log_regression_pred = logistic_regressor.predict(X_validation_set)\n",
        "\n",
        "\n",
        "# In[165]:\n",
        "\n",
        "\n",
        "# In[180]:\n",
        "# # Understanding Accuracy metrics\n",
        "# \n",
        "# <b>1.True Positives (TP):</b> True positives are the cases when the actual class of the data point was 1(True) and the predicted is also 1(True)\n",
        "# Ex: The case where a person is actually having cancer(1) and the model classifying his case as cancer(1) comes under True positive.\n",
        "# \n",
        "# <b>2.True Negatives (TN):</b> True negatives are the cases when the actual class of the data point was 0(False) and the predicted is also 0(False\n",
        "# \n",
        "# Ex: The case where a person NOT having cancer and the model classifying his case as Not cancer comes under True Negatives.\n",
        "# \n",
        "# <b>3.False Positives (FP):</b> False positives are the cases when the actual class of the data point was 0(False) and the predicted is 1(True). False is because the model has predicted incorrectly and positive because the class predicted was a positive one. (1)\n",
        "# \n",
        "# Ex: A person NOT having cancer and the model classifying his case as having cancer comes under False Positives.\n",
        "# \n",
        "# <b>4.False Negatives (FN):</b> False negatives are the cases when the actual class of the data point was 1(True) and the predicted is 0(False). False is because the model has predicted incorrectly and negative because the class predicted was a negative one. (0)\n",
        "# \n",
        "# Ex: A person having cancer and the model classifying his case as No-cancer comes under False Negatives.\n",
        "# \n",
        "# ### Minimization and Trade offs :\n",
        "# \n",
        "# We know that there will be some error associated with every model that we use for predicting the true class of the target variable. This will result in False Positives and False Negatives(i.e Model classifying things incorrectly as compared to the actual class).\n",
        "# \n",
        "# There’s no hard and fast rule that says what should be minimised in all the situations. It purely depends on the business needs and the context of the problem you are trying to solve. Based on that, we might want to minimise either False Positives or False negatives.\n",
        "# \n",
        "# ### Accuracy:\n",
        "# Accuracy in classification problems is the number of correct predictions made by the model over all kinds predictions made.\n",
        "# ![accuracy.png](attachment:accuracy.png)\n",
        "# \n",
        "# \n",
        "# ### Precision:\n",
        "# Precision talks about how precise/accurate the model is out of those predicted positive, how many of them are actual positive.\n",
        "# ![precision.png](attachment:precision.png)\n",
        "# \n",
        "# \n",
        "# ### Recall - True Positive Rate:\n",
        "# What percent of the positive cases did the model catch (predicted positive) amongst all positive cases. Recall actually calculates how many of the Actual Positives our model capture through labeling it as Positive.\n",
        "# ![recall.png](attachment:recall.png)\n",
        "# \n",
        "# \n",
        "# ### False Positive Rate:\n",
        "# <b>False Positive Rate = False Positives / (False Positives + True Negatives) </b>\n",
        "# \n",
        "# \n",
        "# ### F-1 Score:\n",
        "# F1 score (also F-score or F-measure) is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results returned by the classifier, and r is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n",
        "# ![f-1%20score.png](attachment:f-1%20score.png)\n",
        "# \n",
        "# \n",
        "# ### ROC (receiver operating characteristic) Curve:\n",
        "# A curve of true positive rate vs. false positive rate at different classification thresholds.\n",
        "# \n",
        "# ### AUROC (Area under ROC):\n",
        "# An evaluation metric that considers all possible classification thresholds.\n",
        "# \n",
        "# The Area Under the ROC curve is the probability that a classifier will be more confident that a randomly chosen positive example is actually positive than that a randomly chosen negative example is positive.\n",
        "# \n",
        "# image source : https://medium.com/greyatom/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b\n",
        "# \n",
        "# https://en.wikipedia.org/wiki/F1_score\n",
        "# \n",
        "# https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\n",
        "\n",
        "# In[109]:\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score,classification_report, roc_auc_score\n",
        "print(\"The accuracy in general is : \", accuracy_score(y_validation_set,log_regression_pred))\n",
        "print(\"\\n\")\n",
        "print(\"The classification report is as follows:\\n\", classification_report(y_validation_set,log_regression_pred))\n",
        "print(\"ROC AUC score is: \",roc_auc_score(y_validation_set,log_regression_pred))\n",
        "\n",
        "\n",
        "# We want to predict the probabilty of not paying a loan, so we use the model predict.proba method. \n",
        "# This returns an m x 2 array where m is the number of datapoints.\n",
        "# The first column is the probability of the target being 0 and the second column is the probability of the \n",
        "# target being 1. We want the probability the loan is not repaid, so we will select the second column.\n",
        "\n",
        "# In[110]:\n",
        "\n",
        "\n",
        "log_regression_pred_test = logistic_regressor.predict_proba(test_transformed)\n",
        "\n",
        "\n",
        "# In[111]:\n",
        "\n",
        "\n",
        "# selecting the second column\n",
        "log_regression_pred_test[:,1]\n",
        "\n",
        "\n",
        "# In[112]:\n",
        "\n",
        "\n",
        "submission_log_regression = test[['SK_ID_CURR']]\n",
        "submission_log_regression['TARGET'] = log_regression_pred_test[:,1]\n",
        "\n",
        "\n",
        "# In[113]:\n",
        "\n",
        "\n",
        "submission_log_regression.head(10)\n",
        "\n",
        "\n",
        "# In[114]:\n",
        "\n",
        "\n",
        "submission_log_regression.to_csv(\"log_regression.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}